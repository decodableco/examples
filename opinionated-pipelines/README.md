# Opinionated Data Pipelines
Schema-on-write is a feature many engineers always understood used to their advantage when building data pipelines. In this example, we will introduce this idea with Decodableâ€™s opinionated approach to building streaming data pipelines to bring schema-on-write to streaming and big data.

## Getting started
First create a ``.env`` file and place the contents these contents below replacing the values with yours. You can ``source .env`` to put all these values in the environment.

```Makefile
ACCOUNT=<<YOUR DECODABLE ACCOUNT NAME>>
SCHEMA=./schemas/employee.json
CONNECTION=4c377290

# confluent cloud connection
BOOTSTRAP=<<CONFLUENT BOOTSTRAP URL>>
CLUSTER_ID=<<CONFLUENT CLUSTER ID>>
CONFLUENT_KEY=<<CONFLUENT KEY>>
CONFLUENT_SECRET=<<CONFLUENT SECRET>>
TOPIC=<<TOPIC NAME>>
```

First log into Decodable using the Decodable CLI.

```bash
make login
```

### Step 1: The Schema
In the schema directory, there is a JSON schema ([Employee](schemas/employee.json)) that defines a domain entity. 


```json
{
    "$schema": "https://json-schema.org/draft/2020-12/schema",
    "$id": "https://example.com/product.schema.json",
    "title": "Employee",
    "description": "An employee in your company",
    "type": "object",
    "properties": {
        "empid": {
            "description": "Identifier that is not a SSN",
            "type": "integer"
        },
        "SSN": {
            "description": "Identifier that is not a SSN",
            "type": "string"
        }
        ,
        "fname": {
            "description": "First name",
            "type": "string"
        },
        "lname": {
            "description": "Last name",
            "type": "string"
        },
        "address": {
            "description": "Address",
            "type": "string"
        }
    },
    "required": [
        "empid", "SSN", "fname", "lname", "address"
    ]
}
```

This is the schema we expect our pipelines to use when validating data as they come in. You have the option to provide your own schema for testing.

Our first step is to create a stream definition in Decodable that conform to this entity (Employee). A stream is a partitioned, partially-ordered sequence of records on the Decodable platform. They can be used by any number of connections or pipelines as inputs or outputs. Streams present a universal interface to data from different sources, simplifying pipeline development.

We can build all of the entities in our domain model like we define tables in a database, pojos in Java, or models in golang.

Run this command to define our Employee entity as a stream in the Decodable platform.

```bash
make stream
```

### Step 2: Pipeline Assembly
Every component in the Decodable has a schema assigned to it. So when you assemble these components together, they should fit like puzzle pieces. If the schemas don't match, the components cannot be assembled. This opinionated approach to building streaming data pipelines keeps users from assembling components together that will break.


#### Schema Matching Error
Let's start by trying to create an ``invalid`` Confluent Cloud source connection that does not conform to the Employee schema in Decodable. 

```bash
make invalid
```

You will get the error below.

```
decodable conn create \
	--name EMPLOYEE_CONFLUENT \
	--connector confluent-cloud \
	--type source \
	--stream-id 2638d8fe \
	--description "COMMAND for Employee" \
	--field empid=string \
	--field SSN=string \
	--field fname=string \
	--field lname=string \
	--field address=string \
	--prop cluster.api.endpoint=https://pkc-pgq85.us-west-2.aws.confluent.cloud:9092 \
	--prop cluster.id=lkc-38wq12 \
	--prop topic=demo \
	--prop format=json \
	--prop cluster.api.key=NS44ZE3R2PPRGK7Q \
	--prop cluster.api.secret=MvRNfi+lVzWlv6m6tj72trsPT44TMNLCQ1AGPp/1mJZEhb/JShV9anC8NqteprqO
Error: 0001-01-01T00:00:00Z 0: Invalid connection. Reason: Connection schema doesn't match stream schema
Usage:
  decodable connection create [flags]

Aliases:
  create, new

Flags:
      --account string        override the account
      --base-url string       override the base URL
      --conf-file string      specify (or override) the conf file location
      --connector string      connector name (e.g. kafka)
      --description string    connection description
      --field stringArray     schema field definition (name=type) (default )
  -h, --help                  help for create
      --name string           connection name
  -o, --output string         output format (one of json|plain|yaml)
  -p, --profile string        override active profile
      --prop stringToString   set a property (<key>=<value>) (default )
      --stream-id string      stream id
      --type string           connection type (source|sink)

Error: 0001-01-01T00:00:00Z 0: Invalid connection. Reason: Connection schema doesn't match stream schema
make: *** [invalid] Error 1
```

The error is because the line ``--field empid=string`` does not match the Employee schema which defines ``empid=integer``. 

#### Valid Connection
Let's create a REST API source connection to the Employee stream with the correct schema definition. The command below will create a valid source connection.

```bash
make valid
```

Run the command below to activate the connection to the Employee stream.

```bash
 make activate
```

### Step 3: Schema Validation Behaviors
In this step we will see the how schema validation works in the Decodable platform.

Open another terminal window to view the contents of the Employee stream as messages come through.

```bash
make preview
```

Back in the original terminal, publish some good messages to Kafka.

```bash
make publish JSON=data/good-kafka.json
```

You will see in the other terminal the message we just sent come through. Now lets send a bad message.

```bash
make publish JSON=data/bad-kafka.json
```

You will see in the other terminal that the message we just sent doesn't appear. Let's see the status of the source connection.

```bash
make status
```

You will get a status message below.

```
Serialization issue with Avro data. This may be an underlying problem with the payload or the schema registry.
```

## Error Handling
There are other ways of handling this type of error. By default, Decodable will continue to processes good messages and drop bad ones. Decodable will provide the option of a dead letter queue to hold all bad messages the connection encounters as an alternative handling solution.

